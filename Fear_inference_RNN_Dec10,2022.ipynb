{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "gpuClass": "premium"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morningspread/Angry-Tweeter/blob/main/Fear_inference_RNN_Dec10%2C2022.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sp7D0ktn5eiG"
      },
      "source": [
        "# This is our inference notebook \n",
        "## INference Dataset: Badhresh Savani - emotion dataset \n",
        "\n",
        "dataset location: [Tweet Emotion Dataset](https://github.com/dair-ai/emotion_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6N7KIbjNtJlr",
        "outputId": "658dfc52-5a17-44a2-f138-22aac8247f89"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#To do: \n",
        "\n",
        "1.   What parts are necessary / unecessary? List items\n",
        "2.   How do we bring in other datasources? \n",
        "3.   Do we have to...vectorize the tweets...\n",
        "4.   What about the labels? There's some mislabelling happening... \n",
        "\n",
        "#Resources \n",
        "\n",
        "https://www.tensorflow.org/guide/keras/save_and_serialize\n",
        "\n",
        "model = ...  # Get model (Sequential, Functional Model, or Model subclass)\n",
        "model.save('path/to/location')"
      ],
      "metadata": {
        "id": "ajSQQC9naPQL"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dLnDPdYJiJHv"
      },
      "source": [
        "## 1. Installing Hugging Face's nlp package\n",
        "\n",
        "1.   This is needed for importing the Bhadresh Savani dataset... \n",
        "2.   There is another dependency in the import section... \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5agZRy-45i0g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "778c8305-4a47-42cc-d53a-62032a8601c8"
      },
      "source": [
        "#Keep this until we switch out the Bhadresh Savani dataset... \n",
        "\n",
        "!pip install nlp\n",
        "!pip install datasets "
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: nlp in /usr/local/lib/python3.8/dist-packages (0.4.0)\n",
            "Requirement already satisfied: pyarrow>=0.16.0 in /usr/local/lib/python3.8/dist-packages (from nlp) (9.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from nlp) (1.3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.8/dist-packages (from nlp) (1.21.6)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from nlp) (4.64.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from nlp) (3.8.0)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.8/dist-packages (from nlp) (0.3.6)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from nlp) (2.23.0)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from nlp) (3.1.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->nlp) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->nlp) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->nlp) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->nlp) (2022.9.24)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nlp) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->nlp) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->nlp) (1.15.0)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.8/dist-packages (2.7.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.8/dist-packages (from datasets) (0.70.14)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from datasets) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.11.1)\n",
            "Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (2022.11.0)\n",
            "Requirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (9.0.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.8/dist-packages (from datasets) (21.3)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.18.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.8/dist-packages (from datasets) (3.8.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.8/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: dill<0.3.7 in /usr/local/lib/python3.8/dist-packages (from datasets) (0.3.6)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.8/dist-packages (from datasets) (1.3.5)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.8/dist-packages (from datasets) (4.64.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.8/dist-packages (from datasets) (3.1.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.8.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (2.1.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (22.1.0)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.8/dist-packages (from aiohttp->datasets) (6.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.4.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.8.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging->datasets) (3.0.9)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (1.25.11)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2022.9.24)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2022.6)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.8/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.8/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ecz7shv-ClA1"
      },
      "source": [
        "## 2. Importing the libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yKFjWz6e5eiH"
      },
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import nlp\n",
        "import random\n",
        "import pandas as pd"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "826tQBp5zlb1"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's connect to Google Drive\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZFXQCtgQlxYL",
        "outputId": "fb10318c-3cd0-4b76-c720-f3857ba246b6"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nvfVk2GNiJH5"
      },
      "source": [
        "## 3. Importing the Dataset\n",
        "\n",
        "1.   This section could be replaced with another import file.. \n",
        "2.   Canonical workflow: CSV import -> DataFrame -> List of Tweets -> Tokenization\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YHOvjAu5eiL",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "f08989c7-f488-47f8-efd2-832ed6207db6"
      },
      "source": [
        "dataset = nlp.load_dataset('emotion')"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:nlp.builder:Using custom data configuration default\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.09 MiB, post-processed: Unknown sizetotal: 4.06 MiB) to /root/.cache/huggingface/datasets/emotion/default/0.0.0/84e07cd366f4451464584cdbd4958f512bcaddb1e921341e07298ce8a9ce42f4...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ConnectionError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mConnectionError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-4bd952a50131>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'emotion'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, version, data_dir, data_files, split, cache_dir, features, download_config, download_mode, ignore_verifications, save_infos, **config_kwargs)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m     \u001b[0;31m# Download and prepare data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m     builder_instance.download_and_prepare(\n\u001b[0m\u001b[1;32m    549\u001b[0m         \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_verifications\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mignore_verifications\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/builder.py\u001b[0m in \u001b[0;36mdownload_and_prepare\u001b[0;34m(self, download_config, download_mode, ignore_verifications, try_from_hf_gcs, dl_manager, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    460\u001b[0m                         \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Dataset not on Hf google storage. Downloading and preparing it from source\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdownloaded_from_gcs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 462\u001b[0;31m                     self._download_and_prepare(\n\u001b[0m\u001b[1;32m    463\u001b[0m                         \u001b[0mdl_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverify_infos\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mdownload_and_prepare_kwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/builder.py\u001b[0m in \u001b[0;36m_download_and_prepare\u001b[0;34m(self, dl_manager, verify_infos, **prepare_split_kwargs)\u001b[0m\n\u001b[1;32m    516\u001b[0m         \u001b[0msplit_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSplitDict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    517\u001b[0m         \u001b[0msplit_generators_kwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_split_generators_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprepare_split_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 518\u001b[0;31m         \u001b[0msplit_generators\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl_manager\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0msplit_generators_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    519\u001b[0m         \u001b[0;31m# Checksums verification\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mverify_infos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/datasets/emotion/84e07cd366f4451464584cdbd4958f512bcaddb1e921341e07298ce8a9ce42f4/emotion.py\u001b[0m in \u001b[0;36m_split_generators\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_split_generators\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0;34m\"\"\"Returns SplitGenerators.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0mtrain_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TRAIN_DOWNLOAD_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m         \u001b[0mvalid_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_VALIDATION_DOWNLOAD_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0mtest_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdl_manager\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_and_extract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_TEST_DOWNLOAD_URL\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/download_manager.py\u001b[0m in \u001b[0;36mdownload_and_extract\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0mextracted_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextracted\u001b[0m \u001b[0mpaths\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m \u001b[0mURL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \"\"\"\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextract\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_recorded_sizes_checksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/download_manager.py\u001b[0m in \u001b[0;36mdownload\u001b[0;34m(self, url_or_urls)\u001b[0m\n\u001b[1;32m    153\u001b[0m                 \u001b[0murl_or_urls\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[0;32m--> 155\u001b[0;31m         downloaded_path_or_paths = map_nested(\n\u001b[0m\u001b[1;32m    156\u001b[0m             \u001b[0;32mlambda\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    157\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/py_utils.py\u001b[0m in \u001b[0;36mmap_nested\u001b[0;34m(function, data_struct, dict_only, map_list, map_tuple, map_numpy)\u001b[0m\n\u001b[1;32m    189\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m     \u001b[0;31m# Singleton\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_struct\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/download_manager.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m    154\u001b[0m         \"\"\"\n\u001b[1;32m    155\u001b[0m         downloaded_path_or_paths = map_nested(\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0;32mlambda\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcached_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_download_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m         )\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_record_sizes_checksums\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_urls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownloaded_path_or_paths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/file_utils.py\u001b[0m in \u001b[0;36mcached_path\u001b[0;34m(url_or_filename, download_config, **download_kwargs)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mis_remote_url\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl_or_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;31m# URL, so get it from the cache (downloading if necessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 191\u001b[0;31m         output_path = get_from_cache(\n\u001b[0m\u001b[1;32m    192\u001b[0m             \u001b[0murl_or_filename\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    193\u001b[0m             \u001b[0mcache_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcache_dir\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/nlp/utils/file_utils.py\u001b[0m in \u001b[0;36mget_from_cache\u001b[0;34m(url, cache_dir, force_download, proxies, etag_timeout, resume_download, user_agent, local_files_only)\u001b[0m\n\u001b[1;32m    354\u001b[0m                 \u001b[0;34m\" to False.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    355\u001b[0m             )\n\u001b[0;32m--> 356\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mConnectionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Couldn't reach {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    357\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m     \u001b[0;31m# From now on, connected is True.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mConnectionError\u001b[0m: Couldn't reach https://www.dropbox.com/s/1pzkadrvffbqw6o/train.txt?dl=1"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z7eCnxU25eiN"
      },
      "source": [
        "train = dataset['train']\n",
        "val = dataset['validation']\n",
        "test = dataset['test']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Object type: nlp.arrow_dataset.Dataset\n",
        "\n",
        "type(train)"
      ],
      "metadata": {
        "id": "YScnEYXlo_Ws"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see about converting this into a pandas dataframe\n",
        "\n",
        "df = pd.DataFrame(data=train)\n",
        "\n",
        "#Let's look at the shape: \n",
        "\n",
        "print(df.shape)\n",
        "\n",
        "print(type(df))\n",
        "\n",
        "df.head()"
      ],
      "metadata": {
        "id": "WiwS8eTd5CXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Extracting the df to a list... and then use this as an input into a tokenizer... \n",
        "#This seems to work: list with length of 16,000 \n",
        "\n",
        "#tweetcol= df['text'].tolist()"
      ],
      "metadata": {
        "id": "6AR1CcX3tT0a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oDYXMfZy5eiP"
      },
      "source": [
        "#Function: function argument \"data\" is an arrow dataset\n",
        "#Output: the ouptut of the function is a list.. .\n",
        "\n",
        "def get_tweets(data):\n",
        "    tweets = [x['text'] for x in data]\n",
        "    labels = [x['label'] for x in data]\n",
        "    return tweets, labels"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jeq3-vSB5eiR"
      },
      "source": [
        "#The function runs on an arrow objct but not a dataframe... \n",
        "\n",
        "tweets, labels = get_tweets(train)\n",
        "\n",
        "#The function errors on the pandas dataframe object... \n",
        "\n",
        "#tweets, labels = get_tweets(df)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Question: What is the tweet and the labels object? \n",
        "#tweets -> train is a list (16,000) -> labels: a list (16,000)\n",
        "\n",
        "labels?"
      ],
      "metadata": {
        "id": "sBbmqVx0xWeN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHD3Tk0J5eiU"
      },
      "source": [
        "tweets[10], labels[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9guH0-5qiJII"
      },
      "source": [
        "## 4. Tokenizing the Tweets\n",
        "\n",
        "# Input into the tokenizer appears to be a list... verify... \n",
        "\n",
        "#To do - Nov 27, 2022 \n",
        "\n",
        "\n",
        "1.   The input into the tokenizer appears to be a list... \n",
        "2.   What are the \"acceptable\" ranges for the num_words argument? I suppose that we could just play with it. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Importing the tokenizer and pad sequences... "
      ],
      "metadata": {
        "id": "pITGLdeFpL9s"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IYN8LWDmiJII"
      },
      "source": [
        "#Importing the tokenizer... \n",
        "\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "#Importing the pad sequence... \n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Instantiating the tokenizer object...\n",
        "\n",
        "It's a: \"keras.preprocessing.text.Tokenizer\""
      ],
      "metadata": {
        "id": "gNaRLoshocHI"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cckUvwBo5eif"
      },
      "source": [
        "#Hyperparameter: the num_words variable...not sure what \n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000, oov_token='<UNK>')\n",
        "\n",
        "#tokenizing works with either: \"tweets\" -> arrowdataframe or the \"tweetcol\" -> Pandas dataframe?\n",
        "\n",
        "tokenizer.fit_on_texts(tweets)\n",
        "\n",
        "#Testing out the tokenizer... \n",
        "\n",
        "print(tokenizer.texts_to_sequences([tweets[10]]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is the tweet object? \n",
        "#It's a list and each tweet is split by commas and '' \n",
        "\n",
        "#tweets?"
      ],
      "metadata": {
        "id": "dvxu0G8yqTA1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Visualizing the lengths of the tweets - this can vary depending on text extraction..."
      ],
      "metadata": {
        "id": "cAk_DPmrpbaQ"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbDdupDkiJIJ"
      },
      "source": [
        "#For loop for creating the lenghts of the tweets \n",
        "#I'm not really sure how this works except that \n",
        "\n",
        "lengths = [len(t.split(' ')) for t in tweets]\n",
        "\n",
        "#Visualizing the tweet length - create a set object\n",
        "\n",
        "plt.hist(lengths, bins=len(set(lengths)))\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What is lengths? It's a list... \n",
        "#Let's try to convert to a dataframe...\n",
        "\n",
        "a=pd.DataFrame(lengths)\n",
        "a.describe()"
      ],
      "metadata": {
        "id": "_SogXpxG0NWy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5AwQt4AgiJIJ"
      },
      "source": [
        "## 5. Tokeninzing, Truncating and Padding Sequences\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q9J_Iemf5eiq"
      },
      "source": [
        "#The function that tokenizes and pads the tweets... \n",
        "#The function seems to eb taking two arguments: \"tokenizer\" and \"tweets\"\n",
        "\n",
        "def get_sequences(tokenizer, tweets):\n",
        "    sequences = tokenizer.texts_to_sequences(tweets)\n",
        "    padded_sequences = pad_sequences(sequences, truncating='post', maxlen=50, padding='post')\n",
        "    return padded_sequences"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eglH77ky5ei0"
      },
      "source": [
        "#Runing the \"get_sequences\" tokenizer...\n",
        "\n",
        "padded_train_sequences = get_sequences(tokenizer, tweets)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iGR473HA5ei7"
      },
      "source": [
        "padded_train_sequences[10]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ5SSsM4iJIL"
      },
      "source": [
        "## 6. Preparing the Labels\n",
        "\n",
        "Nov 28, 2022: This is the part that doesn't make sense... \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SufT2bpD5ejE"
      },
      "source": [
        "#Where is this object getting pulled from? It's from the function... \n",
        "#And that function is extracting from the dataset that that we no longer need... \n",
        "\n",
        "classes = set(labels)\n",
        "print(classes)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So, then what is the classes object? \n",
        "\n",
        "print(type(classes))"
      ],
      "metadata": {
        "id": "oynKj6lM2_6g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rpwzL88I7YSm"
      },
      "source": [
        "#Looking at the distribution  of the data\n",
        "\n",
        "plt.hist(labels, bins=11)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNLF6rXL5ejN"
      },
      "source": [
        "classes_to_index = dict((c, i) for i, c in enumerate(classes))\n",
        "index_to_classes = dict((v, k) for k, v in classes_to_index.items())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_08InVyM5ejc"
      },
      "source": [
        "classes_to_index"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AJx0hFItiJIO"
      },
      "source": [
        "index_to_classes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jq0WJYsP5ejR"
      },
      "source": [
        "names_to_ids = lambda labels: np.array([classes_to_index.get(x) for x in labels])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v15KnrNC5ejW"
      },
      "source": [
        "train_labels = names_to_ids(labels)\n",
        "print(train_labels[10])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#So what is train_labels then? \n",
        "\n",
        "train_labels?"
      ],
      "metadata": {
        "id": "yjjssNpLhhsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XiU-m6wEiJIT"
      },
      "source": [
        "## 8. Inference"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#loading the trained model - Robin's version \n",
        "#from tensorflow import keras\n",
        "#model = keras.models.load_model(\"/content/drive/MyDrive/my_model\")"
      ],
      "metadata": {
        "id": "gAjoRu3j2JLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Loading the model -> Anger Arousal - Oct 20, 2022 \n",
        "#I'm not sure about this -> are we loading the right model...\n",
        "\n",
        "\n",
        "model = keras.models.load_model(\"/content/drive/MyDrive/Sean/Emoclass_Dec2022/Fear_arousal/Fear_Arousal_Model_6cat_dec10,2022\")"
      ],
      "metadata": {
        "id": "cH7-ppi5mOCS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4xCKeBP1iJIV"
      },
      "source": [
        "#I'm not sure where this thing came from...I think it was Robin...\n",
        "#But I don't think that we want to use test tweets \n",
        "\n",
        "#test_tweets, test_labels = get_tweets(test)\n",
        "#test_sequences = get_sequences(tokenizer, test_tweets)\n",
        "#test_labels = names_to_ids(test_labels)\n",
        "\n",
        "\n",
        "#So let's duplicate the code and run it down here... \n",
        "\n",
        "train_tweets, test_labels = get_tweets(train)\n",
        "train_sequences = get_sequences(tokenizer, train_tweets)\n",
        "train_labels = names_to_ids(train_labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Inference"
      ],
      "metadata": {
        "id": "hBi4MQ0OQvQf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_i9LUUDsiJIW"
      },
      "source": [
        "#preds = model.predict_classes(test_sequences)\n",
        "preds=model.predict(train_sequences) \n",
        "classes_x=np.argmax(preds,axis=1)\n",
        "preds.shape, train_labels.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inf_df=pd.DataFrame({\n",
        "    'data':train_tweets,\n",
        "    \"labels_predicted\": classes_x                    \n",
        "})\n",
        "inf_df[\"label_marked\"]=inf_df['labels_predicted'].apply(lambda x: index_to_classes[x])\n",
        "inf_df"
      ],
      "metadata": {
        "id": "OoArtSuSSdZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#What does our inf_df object look like? \n",
        "\n",
        "inf_df.shape"
      ],
      "metadata": {
        "id": "Aw95ExhEmFRE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#We have an inference dataset... \n",
        "\n",
        "Oct 20, 2022: Before we dive into anything here...let's see what we have wtr to classifications / classes. This is one part of the code that doesn't make a lot of sense to me -> i.e. I need to better understand it. "
      ],
      "metadata": {
        "id": "EGPX5d9gvly0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's see what it looks like...\n",
        "\n",
        "inf_df.head()"
      ],
      "metadata": {
        "id": "5WgYd2TqjSkO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inf_df['labels_predicted'].value_counts()"
      ],
      "metadata": {
        "id": "aphXl_EJvgr6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's look at the frequency of these...REmember -> I don't think that this label is working very well...\n",
        "inf_df['label_marked'].value_counts()"
      ],
      "metadata": {
        "id": "IfrX7PNAClSo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Let's export the dataset... \n",
        "\n",
        "/content/drive/MyDrive/Sean/Emoclass_Dec2022/Fear_arousal\n",
        "\n",
        "filepath = =('/content/drive/MyDrive/Sean/Emoclass_Dec2022/Fear_arousal/Fear_arousal_inference_bsavani_Dec10,2022') \n",
        "\n",
        "df.to_csv(filepath)  \n"
      ],
      "metadata": {
        "id": "lWkOTzfdio6q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Note Dec 5, 2022 \n",
        "## We may add some diagnostic code to this workbook. But for now, let's just work with this codebook. As a general strategy, we should be focusing on making codebooks more concise / simpler... "
      ],
      "metadata": {
        "id": "dFFaU-uqSM4j"
      }
    }
  ]
}