{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Morningspread/Angry-Tweeter/blob/main/Train_Fear_Distil_TF_May20%2C2023.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aukch50HAYvQ"
      },
      "source": [
        "#Trying out a pre-trained language model\n",
        "\n",
        "The issues that we are going to need to address are:\n",
        "\n",
        "\n",
        "1.   What are compute requirements... \n",
        "2.   How does the whole tokenization / embedding thing work... \n",
        "3.   What training set(s) to use? \n",
        "4.   What model to use that doesn't mess with our inference? \n",
        "\n",
        "\n",
        "Resources: \n",
        "https://www.tensorflow.org/text/tutorials/classify_text_with_bert\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WuIGmtb_B7C_"
      },
      "source": [
        "#Import libraries "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow\n",
        "!pip install transformers"
      ],
      "metadata": {
        "id": "2ycJKemnJrKV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c8fb139c-bf79-441b-b2ca-85a20e72be17"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.12.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.3.3)\n",
            "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.54.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: jax>=0.3.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.10)\n",
            "Requirement already satisfied: keras<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.0)\n",
            "Requirement already satisfied: numpy<1.24,>=1.22 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.22.4)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: tensorboard<2.13,>=2.12 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.13,>=2.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.12.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.32.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.40.0)\n",
            "Requirement already satisfied: ml-dtypes>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: scipy>=1.7 in /usr/local/lib/python3.10/dist-packages (from jax>=0.3.15->tensorflow) (1.10.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.17.3)\n",
            "Requirement already satisfied: google-auth-oauthlib<1.1,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.0.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.27.1)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (0.7.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (1.8.1)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.13,>=2.12->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (5.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.13,>=2.12->tensorflow) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.13,>=2.12->tensorflow) (2.1.2)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.13,>=2.12->tensorflow) (0.5.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.13,>=2.12->tensorflow) (3.2.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import DistilBertTokenizerFast, TFDistilBertForSequenceClassification"
      ],
      "metadata": {
        "id": "4-MRQxa7Jc_Q"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "c20N-V31A92x"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import pandas as pd\n",
        "import pickle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9j5_Fhx7BFrL"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from transformers import BertTokenizer, TFBertModel\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.optimizers import Adam\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3ui5H4hMA_kN"
      },
      "outputs": [],
      "source": [
        "#Dec 7, 2022: Not sure what this piece of code is used for... \n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "xfooO6CTBBN4"
      },
      "outputs": [],
      "source": [
        "#Setting column options\n",
        "\n",
        "pd.set_option('display.width', 200)\n",
        "pd.set_option('display.max_columns', 100)\n",
        "\n",
        "#Supressing the scientific notation \n",
        "\n",
        "pd.set_option('display.float_format', lambda x: '%.3f' % x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JYeLQ-vkNkL6"
      },
      "source": [
        "#Import the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NgS7YqfNi6O",
        "outputId": "455e6769-5c5a-49f7-e8ca-79e3ea678ddd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Load the Drive helper and mount\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "NSvlLSzCNi2z"
      },
      "outputs": [],
      "source": [
        "#Older dataset\n",
        "\n",
        "#path = \"/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Fear_Twit_Kaggle_110K_May9,2023.csv\"\n",
        "#df = pd.read_csv(path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Trying new dataset \n",
        "\n",
        "path = '/content/drive/MyDrive/Sean/Emoclass/Emotion datasets/Fear_Gut55K_2.8mil_June2.csv'\n",
        "df = pd.read_csv(path)"
      ],
      "metadata": {
        "id": "nkkJVO32acmD"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XZ9yr9RrzvaI",
        "outputId": "7f59e179-6fdd-4baa-e46a-3c100b8502b3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3241651, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "2iHoWkdx2Fii"
      },
      "outputs": [],
      "source": [
        "# Create a 50% sample of the DataFrame\n",
        "#df = df.sample(frac=0.5, random_state=42)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "PV7owrXuqjbi"
      },
      "outputs": [],
      "source": [
        "#Relabelling a column... \n",
        "\n",
        "df.rename(columns={\"emotion2\": \"label\"},inplace =True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lh_81iVKCDc_"
      },
      "source": [
        "#Import the emotion axis "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "z0PjqxpPCBCB"
      },
      "outputs": [],
      "source": [
        "#Defining the emotion axis - note the use of the ordered dict here... \n",
        "\n",
        "from collections import OrderedDict\n",
        "\n",
        "emo_axis = OrderedDict([\n",
        "    (\"dread\", ['panic','dread','horror','horrified','horrifying','terror','terrified','terrifying']),\n",
        "    (\"fear\", [\"fear\", \"fearful\", \"fright\",\"frightening\",\"afraid\",\"frightful\",\"frightfully\",\"frightened\",'scared','scary','scare']),\n",
        "    (\"anxiety\", [\"anxious\",\"anxiety\",'angst','anxiousness']),\n",
        "    (\"worry\", [\"worry\", \"worried\",\"worrying\",\"worries\"]),\n",
        "    (\"concern\", [\"concern\", \"concerning\",\"concerned\",\"concerns\"]),\n",
        "    (\"calm\", [\"calm\",\"peaceful\",\"serene\",\"serenity\",\"untroubled\", \"content\", \"contented\", \"composed\", \"tranquil\"])\n",
        "])\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "ffzaZ-L6Lbsw"
      },
      "outputs": [],
      "source": [
        "# Assign numerical labels to emotion categories in reversed order\n",
        "labels = list(emo_axis.keys())\n",
        "label_mapping = {label: i for i, label in enumerate(reversed(labels))}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql9xArvAMRuD",
        "outputId": "8e09b65c-9137-484c-b3f8-e2ab789b71af"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'calm': 0, 'concern': 1, 'worry': 2, 'anxiety': 3, 'fear': 4, 'dread': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "#check the label mapping \n",
        "\n",
        "label_mapping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8xWiQxmYOeJH"
      },
      "source": [
        "#Remove key emotion words "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "tegOfEDGOXqZ"
      },
      "outputs": [],
      "source": [
        "#check the right label is used...\n",
        "\n",
        "#df['emotion1'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HYgaB6s1OXtG"
      },
      "outputs": [],
      "source": [
        "#Generate all the unique emotion words that then get replaced... \n",
        "\n",
        "a = df['emotion1'].unique().tolist()\n",
        "\n",
        "#The list a is our list of variable responses from the dataset... \n",
        "keyword = \"emotions\"\n",
        "words = a\n",
        "for j in words: \n",
        "  df['text'] = df['text'].str.replace(j,keyword)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yl1VEYYoOmd_"
      },
      "source": [
        "##Splitting into train, validate and test "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "i2cuyYUxOl0x"
      },
      "outputs": [],
      "source": [
        "#This outputs 3 different dataframes... originally 0.6 and 0.8\n",
        "\n",
        "train, validate, test = np.split(df.sample(frac=1, random_state=42),\n",
        "                       [int(.8*len(df)), int(.9*len(df))])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iMCCYYZxOyOC"
      },
      "source": [
        "##Preparing the Labels "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "3qy4bp-T5fun"
      },
      "outputs": [],
      "source": [
        "#Converting the pandas dataframe into a list of labels... \n",
        "#We may consider puting this into a function... \n",
        "\n",
        "trainlabel=train['label'].tolist()\n",
        "vallabel=validate['label'].tolist()\n",
        "testlabel=test['label'].tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x6kEkjTSO_ym",
        "outputId": "7b1dd5d0-c936-47d9-9886-b284ba663b9d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'calm': 0, 'concern': 1, 'worry': 2, 'anxiety': 3, 'fear': 4, 'dread': 5}"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "#Test the label mapping again - dyslexic after all... \n",
        "\n",
        "label_mapping "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "AXvZsD_k5fsA"
      },
      "outputs": [],
      "source": [
        "# Convert the train, validation, and test labels using label_mapping\n",
        "train_labels = [label_mapping[label] for label in trainlabel]\n",
        "val_labels = [label_mapping[label] for label in vallabel]\n",
        "test_labels = [label_mapping[label] for label in testlabel]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "K8oi-4A4O2st"
      },
      "outputs": [],
      "source": [
        "#Convert labels to Tensors (int32) \n",
        "    \n",
        "train_labels = tf.convert_to_tensor(train_labels, dtype=tf.int32)\n",
        "val_labels = tf.convert_to_tensor(val_labels, dtype=tf.int32)\n",
        "test_labels = tf.convert_to_tensor(test_labels, dtype=tf.int32)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "dIBka6HmNXsd"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVCWqLpMPlR2"
      },
      "source": [
        "#Training and Tokenizing Tweets\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "QljYATnUO2vi"
      },
      "outputs": [],
      "source": [
        "#Converting the different pandas dataframes into a list of text fields...\n",
        "#Choice of variables: truncated_text, text_minus1, 'filtered_text',''filtered_text_minus1'\n",
        "\n",
        "traintext=train['text'].tolist()\n",
        "valtext=validate['text'].tolist()\n",
        "testtext=test['text'].tolist()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "veDB0wGqQGv-"
      },
      "source": [
        "##Load the Tokenizer "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "o-aLuB0h4CPL"
      },
      "outputs": [],
      "source": [
        "#Load the tokenizer...\n",
        "\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Tokenize the data...\n",
        "\n",
        "train_encodings = tokenizer(traintext, truncation=True, padding=True)\n",
        "val_encodings = tokenizer(valtext, truncation=True, padding=True)\n",
        "test_encodings = tokenizer(testtext, truncation=True, padding=True)"
      ],
      "metadata": {
        "id": "-1Lo0U2pKmCZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create tensoflow datasets\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(val_encodings),\n",
        "    val_labels\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n"
      ],
      "metadata": {
        "id": "BPc9rPx7KmFn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1YwDl5rvn3Te"
      },
      "source": [
        "# Load compile and Train the Model "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Load the model\n",
        "\n",
        "model = TFDistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=6)"
      ],
      "metadata": {
        "id": "s2rvJFLQSyzn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nlUHnxEfnhif"
      },
      "outputs": [],
      "source": [
        "#Compile the model \n",
        "\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5), loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Fit the model... \n",
        "\n",
        "model.fit(train_dataset.shuffle(1000).batch(16), epochs=3, batch_size=16, validation_data=val_dataset.batch(16))"
      ],
      "metadata": {
        "id": "ZBRZPrU6LsWt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluate the model "
      ],
      "metadata": {
        "id": "X4GVqnaZM0UM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(test_dataset.batch(16))"
      ],
      "metadata": {
        "id": "_MiRFglBM2g-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Make predictions on test data\n",
        "\n",
        "predictions = model.predict(test_dataset.batch(16))\n"
      ],
      "metadata": {
        "id": "DZNmiRdoNv_A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "\n",
        "#Convert prediction score to predicted classes\n",
        "\n",
        "predicted_classes = tf.argmax(predictions.logits, axis=-1)\n",
        "\n",
        "predicted_classes = tf.cast(tf.argmax(predictions.logits, axis=-1), dtype=tf.int32)\n",
        "\n",
        "#Calculate absolute difference\n",
        "\n",
        "abs_diffs = tf.abs(predicted_classes - test_labels)"
      ],
      "metadata": {
        "id": "UCFxUPdUQL8e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Create histogram of absolute differences \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.hist(abs_diffs.numpy(), bins=11)\n",
        "plt.xlabel('Absolute difference')\n",
        "plt.ylabel('Frequency')\n",
        "plt.title('Distribution of absolute differences between predicted and true classes')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "zEVWSbz8OEc7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Save the tokenizer and the model"
      ],
      "metadata": {
        "id": "WjKTIPBUOKRU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#path = /content/drive/MyDrive/Sean/Emoclass\n",
        "\n",
        "# Save the model\n",
        "model.save_pretrained('/content/drive/MyDrive/Sean/Emoclass/Model_Fear_Twitter_100K_Distill_May20,2023')\n",
        "\n",
        "# Save the tokenizer\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Sean/Emoclass/Distill_Tokenizer_May20')\n"
      ],
      "metadata": {
        "id": "eZ2mz2OzONIn"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyM/N2YXNns1rpR5tyDkAf5s",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}